{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "517_Project_Predictor_MAIN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xhxKhufK2P9"
      },
      "source": [
        "full_data_folder = '/content/drive/MyDrive/MEAM 517 Project/Training_Data/fullData.npy'\n",
        "save_nn_models_folder = '/content/drive/MyDrive/MEAM 517 Project/saved_models'\n",
        "save_numpy_files_folder = '/content/drive/MyDrive/MEAM 517 Project/saved_numpy'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKBaAh5ifaxI"
      },
      "source": [
        "CrossVal = False\n",
        "Training = True #training after cross val. False if trained neural net saved"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzRqrB2tCPJy"
      },
      "source": [
        "import torch\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pRSwnHfqCgbC",
        "outputId": "2499dc0f-1737-4029-a06b-c11394b01543"
      },
      "source": [
        "np.random.seed(1234)\n",
        "torch.manual_seed(1234)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f3aac4babb8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qAauKoFANT9d",
        "outputId": "db02f9de-229e-4c44-c873-92966959af98"
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cpu\")\n",
        "    #device = torch.device(\"cuda:0\") \n",
        "    print(\"Running on the GPU\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"Running on the CPU\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running on the GPU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oiThnBxeX3F9"
      },
      "source": [
        "# **NEURAL NETWORK**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10lIzpgQ_43-"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmTWsvKS_f69"
      },
      "source": [
        "class NETWORK(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, n_layers=None, hidden_layer_size=None):\n",
        "        super(NETWORK, self).__init__()\n",
        "\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_layer_size = hidden_layer_size\n",
        "\n",
        "        self.hidden = nn.ModuleList()\n",
        "        h_sizes = self.hidden_layer_set(input_dim, output_dim)\n",
        "\n",
        "        for k in range(len(h_sizes)-2):\n",
        "            self.hidden.append(nn.Linear(h_sizes[k], h_sizes[k+1]))\n",
        "\n",
        "        self.out = nn.Linear(h_sizes[-2], h_sizes[-1])\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.hidden:\n",
        "            x = F.relu(layer(x))\n",
        "        horizon_length_pred = self.out(x)\n",
        "        return horizon_length_pred\n",
        "    \n",
        "    def hidden_layer_set(self, input_dim, output_dim):\n",
        "        h_sizes = [input_dim]\n",
        "        for i in range(self.n_layers):\n",
        "            h_sizes.append(self.hidden_layer_size)\n",
        "        h_sizes.append(output_dim)\n",
        "        return h_sizes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFx5NBofoJ3U"
      },
      "source": [
        "#**OPTIMIZATION**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t78d2W1oSHyk"
      },
      "source": [
        "from torch.optim import Adam\n",
        "from time import time\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "from torch.utils.data import random_split, TensorDataset, DataLoader\n",
        "from sklearn.model_selection import KFold"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R32VTzk2oYqN"
      },
      "source": [
        "class TRAIN():\n",
        "    def __init__(self, input_dim, output_dim, max_epochs, state_dict_folder, n_layers_set=None, \n",
        "                 hidden_layer_size_set=None, lr_set=None, lr_reduction_set=None, \n",
        "                 patience_set=None, batch_size_set=None, network=None, n_layers_optimal=None, \n",
        "                 hidden_layer_size_optimal=None, lr_optimal=None, lr_reduction_optimal=None, \n",
        "                 patience_optimal=None, batch_size_optimal=None):\n",
        "        self.net = network\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.max_epochs = max_epochs\n",
        "\n",
        "        self.optimizer = None\n",
        "        self.net_state_path = state_dict_folder+'/zador.pth'\n",
        "        self.error_tracker = 10000 \n",
        "\n",
        "        self.n_layers_set = n_layers_set\n",
        "        self.hidden_layer_size_set = hidden_layer_size_set\n",
        "        self.lr_set = lr_set\n",
        "        self.lr_reduction_set = lr_reduction_set\n",
        "        self.patience_set = patience_set\n",
        "        self.batch_size_set = batch_size_set\n",
        "\n",
        "        self.n_layers_optimal = n_layers_optimal \n",
        "        self.hidden_layer_size_optimal = hidden_layer_size_set\n",
        "        self.lr_optimal = lr_optimal\n",
        "        self.lr_reduction_optimal = lr_reduction_optimal\n",
        "        self.patience_optimal = patience_optimal\n",
        "        self.batch_size_optimal = batch_size_optimal\n",
        "\n",
        "        self.lr=None\n",
        "\n",
        "        try:\n",
        "            self.CV_error_tracker = np.zeros(( len(n_layers_set), len(hidden_layer_size_set), len(lr_set), len(lr_reduction_set), len(patience_set), len(batch_size_set) ))\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    def train(self, X, y, train_split, n_layers, hidden_layer_size, lr, lr_reduction, patience, batch_size, print_train=True):\n",
        "        self.lr = lr\n",
        "        self.net = NETWORK(self.input_dim, self.output_dim, n_layers, hidden_layer_size).to(device)\n",
        "        self.optimizer = Adam(self.net.parameters(), lr=self.lr)\n",
        "        epoch_worsening_count = 0\n",
        "\n",
        "        train_loader, validation_loader = self.generate_data_loader(X, y, batch_size, train_split)\n",
        "\n",
        "\n",
        "        for epoch in range(self.max_epochs):\n",
        "            for _, data in enumerate(tqdm(train_loader) if print_train else train_loader):\n",
        "                \n",
        "                inputs, target = data\n",
        "\n",
        "                self.optimizer.zero_grad()\n",
        "                prediction = self.net(inputs.float().to(device))\n",
        "                loss = self.loss_function(prediction, target.to(device).float())\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "                            \n",
        "         \n",
        "            if epoch % patience == 0:\n",
        "                epoch_mse = self.model_error(validation_loader)\n",
        "                if print_train: print('OLD MSE:', self.error_tracker, '\\nCURRENT MSE:', epoch_mse)\n",
        "                if epoch_mse<self.error_tracker:\n",
        "                    epoch_worsening_count = 0\n",
        "                    if print_train: print('MODEL WAS IMPROVED\\n', 'CURRENT LEARNING RATE:', self.lr) \n",
        "                    self.save_checkpoint(epoch_mse, self.lr, self.net.state_dict(), self.net_state_path)\n",
        "                else:\n",
        "                    epoch_worsening_count += 1\n",
        "                    if print_train: print('MODEL WORSENED -> RESET MODEL\\n', 'NEW LEARNING RATE:', self.lr*lr_reduction) \n",
        "                    self.load_checkpoint(self.net_state_path)\n",
        "                    self.save_checkpoint(self.error_tracker, self.lr*lr_reduction, self.net.state_dict(), self.net_state_path)\n",
        "                    self.lr = self.lr*lr_reduction\n",
        "                    self.optimizer = Adam(self.net.parameters(), lr=self.lr) #error\n",
        "\n",
        "                if epoch_worsening_count > 3:\n",
        "                    print('Model Trained (Early Stopping).')\n",
        "                    break\n",
        "            if epoch == (self.max_epochs-1): print('Model Trained (Max Epochs).')\n",
        "        self.error_tracker = 10000 \n",
        "\n",
        "\n",
        "    def cross_validation(self, n_splits, X, y):\n",
        "        kf = KFold(n_splits = n_splits)\n",
        "        num_models_cv = len(n_layers_set) * len(hidden_layer_size_set) * len(lr_set) * len(lr_reduction_set) * len(patience_set) * len(batch_size_set)\n",
        "        cv_i = 0\n",
        "        for par1, n_layers in enumerate(self.n_layers_set):\n",
        "            for par2, hidden_layer_size in enumerate(self.hidden_layer_size_set):\n",
        "                for par3, lr in enumerate(self.lr_set):\n",
        "                    for par4, lr_reduction in enumerate(self.lr_reduction_set):\n",
        "                        for par5, patience in enumerate(self.patience_set):\n",
        "                            for par6, batch_size in enumerate(self.batch_size_set):\n",
        "                                CV_error = 0\n",
        "                                for train_index, test_index in kf.split(X):\n",
        "                                    X_train, X_test = X[train_index], X[test_index]\n",
        "                                    y_train, y_test = y[train_index], y[test_index]\n",
        "                                    self.train(X, y, train_split=0.8, n_layers=n_layers, hidden_layer_size=hidden_layer_size, lr=lr, lr_reduction=lr_reduction, patience=patience, batch_size=batch_size, print_train=False)\n",
        "                                    test_loader = self.generate_data_loader(X_test, y_test, batch_size)\n",
        "                                    CV_error_i = self.model_error(test_loader)\n",
        "                                    CV_error += CV_error_i\n",
        "                                self.CV_error_tracker[par1, par2, par3, par4, par5, par6] = CV_error / n_splits\n",
        "                                cv_i += 1\n",
        "                                print('(', cv_i, '/', num_models_cv, ')     CV-Error:', CV_error / n_splits)\n",
        "\n",
        "        max_par1, max_par2, max_par3, max_par4, max_par5, max_par6 = np.unravel_index(np.argmin(self.CV_error_tracker), self.CV_error_tracker.shape)\n",
        "\n",
        "        self.n_layers_optimal = self.n_layers_set[max_par1] \n",
        "        self.hidden_layer_size_optimal = self.hidden_layer_size_set[max_par2]\n",
        "        self.lr_optimal = self.lr_set[max_par3]\n",
        "        self.lr_reduction_optimal = self.lr_reduction_set[max_par4]\n",
        "        self.patience_optimal = self.patience_set[max_par5]\n",
        "        self.batch_size_optimal = self.batch_size_set[max_par6]\n",
        "                                    \n",
        "    def loss_function(self, prediction, target):\n",
        "        loss = nn.MSELoss()\n",
        "        return loss(prediction.float(), target.float())\n",
        "\n",
        "    def model_error(self, model_test_loader):\n",
        "        MSE = 0\n",
        "        for data in model_test_loader:\n",
        "            X, y = data\n",
        "            prediction = self.net(X.float().to(device))\n",
        "            MSE += np.sum( (prediction.detach().cpu().numpy()-y.detach().cpu().numpy())**2 ) \n",
        "        MSE = MSE / len(model_test_loader.dataset)\n",
        "\n",
        "        return MSE**0.5\n",
        "\n",
        "    def save_checkpoint(self, error, learning_rate, model_state_dict, PATH):\n",
        "        torch.save({'error_tracker': error,\n",
        "                    'learning_rate': learning_rate,\n",
        "                    'net_state_dict': model_state_dict\n",
        "                    }, PATH)\n",
        "        self.load_checkpoint(PATH)\n",
        "        \n",
        "    def load_checkpoint(self, PATH):\n",
        "        checkpoint = torch.load(PATH)\n",
        "        self.net.load_state_dict(checkpoint['net_state_dict'])\n",
        "        self.error_tracker = checkpoint['error_tracker']\n",
        "        self.lr = checkpoint['learning_rate']\n",
        "    \n",
        "    def generate_data_loader(self, X, y, batch_size, train_split=None):\n",
        "        dataset = TensorDataset(torch.tensor(X), torch.tensor(y))\n",
        "        \n",
        "        try:\n",
        "            dataset_size = X.shape[0]\n",
        "            train_size = int(train_split * dataset_size)\n",
        "            split_size = dataset_size - train_size\n",
        "            data_set1, data_set2 = random_split(dataset, [train_size, split_size])\n",
        "            data_loader1 = DataLoader(data_set1, batch_size=batch_size, shuffle=True)\n",
        "            data_loader2 = DataLoader(data_set2, batch_size=batch_size, shuffle=False)\n",
        "            return data_loader1, data_loader2\n",
        "        except:\n",
        "            data_loader1 = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "            return data_loader1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mx3M0yTleT6h"
      },
      "source": [
        "#**MAIN**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BxdxCj9gx-Oo"
      },
      "source": [
        "##DATA PREPERATION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xsPhbJgZzKMg"
      },
      "source": [
        "data = np.load(full_data_folder)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GgRSMu-KvoDN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a2b642f-2241-44a9-dd33-f993021fddd1"
      },
      "source": [
        "rows_with_nan = []\n",
        "for i in range(data.shape[0]):\n",
        "    if np.isnan(data[i, :]).any(): rows_with_nan.append(i)\n",
        "data = np.delete(data, rows_with_nan, 0)\n",
        "\n",
        "X, y = data[:, :-3], data[:, -1]\n",
        "y = y[:, np.newaxis]\n",
        "print(X.shape, y.shape)\n",
        "print('NUMBER OF FEATURES:', X.shape[1])\n",
        "print('      DATASET SIZE:', X.shape[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(43200, 52) (43200, 1)\n",
            "NUMBER OF FEATURES: 52\n",
            "      DATASET SIZE: 43200\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NqCERqqruHxT"
      },
      "source": [
        "##CROSS VALIDATION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYVk1XdgYYII"
      },
      "source": [
        "CROSS VALIDATION PARAMETERS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mT9ceJOuZfW"
      },
      "source": [
        "input_dim = X.shape[1]\n",
        "output_dim = y.shape[1]\n",
        "max_epochs = 50\n",
        "state_dict_folder = save_nn_models_folder\n",
        "\n",
        "n_layers_set = [1, 2, 3]\n",
        "hidden_layer_size_set = [50, 100, 200]\n",
        "lr_set = [0.001]\n",
        "lr_reduction_set = [1/5, 1/10]\n",
        "patience_set = [1, 3]\n",
        "batch_size_set = [64, 128, 256]\n",
        "n_splits = 3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYzbLaSYYbII"
      },
      "source": [
        "CROSS VALIDATION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zChgihCvuKM-"
      },
      "source": [
        "if CrossVal:\n",
        "    train = TRAIN(input_dim=input_dim, output_dim=output_dim, max_epochs=max_epochs, \n",
        "                state_dict_folder=state_dict_folder, n_layers_set=n_layers_set, \n",
        "                hidden_layer_size_set=hidden_layer_size_set, lr_set=lr_set, lr_reduction_set=lr_reduction_set, \n",
        "                patience_set=patience_set, batch_size_set=batch_size_set)\n",
        "\n",
        "    train.cross_validation(n_splits, X, y)\n",
        "\n",
        "    print('  Optimal Number of Layer :', train.n_layers_optimal)\n",
        "    print('      Optimal Layer Sizes :', train.hidden_layer_size_optimal)\n",
        "    print('       Optimal Initial LR :', train.lr_optimal)\n",
        "    print('Optimal LR Reduction Rate :', train.lr_reduction_optimal)\n",
        "    print('         Optimal Patience :', train.patience_optimal)\n",
        "    print('        Optimal Bach Size :', train.batch_size_optimal)\n",
        "    np.save('/content/drive/MyDrive/MEAM517_project/saved_numpy/' + 'optimal_hyp.npy', np.array([train.n_layers_optimal,\n",
        "                                                                                                train.hidden_layer_size_optimal,\n",
        "                                                                                                train.lr_optimal,\n",
        "                                                                                                train.lr_reduction_optimal,\n",
        "                                                                                                train.patience_optimal,\n",
        "                                                                                                train.batch_size_optimal]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHq-w4kJfJ67"
      },
      "source": [
        "##TRAINING"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGImOP6fz662"
      },
      "source": [
        "**COLLECTING CV FINDINGS**\n",
        "- CV was split across the selected batch size to run in parallel \n",
        "- Saved information is:\n",
        "    - Cross validation error of entire process\n",
        "    - Minimum cross validation error of entire process\n",
        "    - Optimal hyperparameters of entire process\n",
        "- Cross vaildation information:\n",
        "    - Hyperparameters:\n",
        "            n_layers_set = [1, 2, 3]\n",
        "            hidden_layer_size_set = [50, 100, 200]\n",
        "            lr_set = [0.001]\n",
        "            lr_reduction_set = [1/5, 1/10]\n",
        "            patience_set = [1, 3]\n",
        "            batch_size_set = [64, 128, 256]\n",
        "    - {CV1, CV2, CV3} : batch_size_set{64, 128, 256}\n",
        "\n",
        "        -> CV split accordingly"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJnE7Ducz5j7"
      },
      "source": [
        "CV1 = np.load(save_numpy_files_folder+ '/Griffon_2_CV_Scores.npy')\n",
        "Min1 = np.load(save_numpy_files_folder+ '/Griffon_2_Minimum_Error.npy')\n",
        "Opt1 = np.load(save_numpy_files_folder+ '/Griffon_2_optimal_hyp.npy')\n",
        "Opt1 = np.delete(Opt1, 2, 0)\n",
        "\n",
        "CV2 = np.load(save_numpy_files_folder+ '/Zador_1_CV_Scores.npy')\n",
        "Min2 = np.load(save_numpy_files_folder+ '/Zador_1_Minimum_Error.npy')\n",
        "Opt2 = np.load(save_numpy_files_folder+ '/Zador_1optimal_hyp.npy')\n",
        "Opt2 = np.delete(Opt2, 2, 0)\n",
        "\n",
        "CV3 = np.load(save_numpy_files_folder+ '/Zador_2_CV_Scores.npy')\n",
        "Min3 = np.load(save_numpy_files_folder+ '/Zador_2_Minimum_Error.npy')\n",
        "Opt3 = np.load(save_numpy_files_folder+ '/Zador_2optimal_hyp.npy')\n",
        "Opt3 = np.delete(Opt3, 2, 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "On7w-EWxg_wQ",
        "outputId": "db183b8f-e97f-4bc5-e945-130ad374c1b2"
      },
      "source": [
        "min1 = np.min(CV1[0, :, :, :, :, :])\n",
        "min1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.3231479416877929"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bckJiUrjhna3",
        "outputId": "4675599f-d222-4d4a-ccc3-188d50c4e4dd"
      },
      "source": [
        "min2 = np.min(CV2[0, :, :, :, :, :])\n",
        "min2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.35635475745214423"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qe4bqbhzhrPQ",
        "outputId": "e955a3d2-0925-43e9-9045-fd60e260f82d"
      },
      "source": [
        "min3 = np.min(CV3[0, :, :, :, :, :])\n",
        "min3"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.33633621671847225"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kfbjFwA6hvvc",
        "outputId": "28d706f8-5297-4a83-8a4c-8fdc04f8e836"
      },
      "source": [
        "argmin1 =  np.unravel_index(np.argmin(CV1[0, :, :, :, :, :]), CV1[0, :, :, :, :, :].shape)\n",
        "argmin1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2, 0, 0, 1, 0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0Z_RDbm54ju"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "UUwARWvc4MOc",
        "outputId": "14fc144a-477d-4139-e6ff-777fc7f105eb"
      },
      "source": [
        "Opt_set = np.vstack((Opt1, Opt2, Opt3))\n",
        "Opt_set = Opt_set.T\n",
        "Opt_df_set = pd.DataFrame(data=Opt_set, index = ['Number of Layers', 'Layer Size', 'LR Reuction Rate', 'Patience', 'Batch Size'], columns=['Opt1', 'Opt2', 'Opt3'])\n",
        "Opt_df_set"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Opt1</th>\n",
              "      <th>Opt2</th>\n",
              "      <th>Opt3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Number of Layers</th>\n",
              "      <td>3.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Layer Size</th>\n",
              "      <td>200.0</td>\n",
              "      <td>200.0</td>\n",
              "      <td>200.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>LR Reuction Rate</th>\n",
              "      <td>0.2</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Patience</th>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Batch Size</th>\n",
              "      <td>64.0</td>\n",
              "      <td>128.0</td>\n",
              "      <td>256.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                   Opt1   Opt2   Opt3\n",
              "Number of Layers    3.0    2.0    2.0\n",
              "Layer Size        200.0  200.0  200.0\n",
              "LR Reuction Rate    0.2    0.1    0.2\n",
              "Patience            3.0    3.0    3.0\n",
              "Batch Size         64.0  128.0  256.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "q2NOqfsP68X3",
        "outputId": "47ec6873-9261-4ad3-aeac-84b40a3093b1"
      },
      "source": [
        "Min_set = np.hstack((Min1, Min2, Min3))\n",
        "Min_set = Min_set[np.newaxis, :]\n",
        "Min_df = pd.DataFrame(data=Min_set, index = [''], columns=['Opt1', 'Opt2', 'Opt3'])\n",
        "Min_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Opt1</th>\n",
              "      <th>Opt2</th>\n",
              "      <th>Opt3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <td>0.230546</td>\n",
              "      <td>0.26166</td>\n",
              "      <td>0.293718</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      Opt1     Opt2      Opt3\n",
              "  0.230546  0.26166  0.293718"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "WJVHSyVM8Ldr",
        "outputId": "17e14aa8-4727-4749-fa9a-8a9c9a8cbd96"
      },
      "source": [
        "opt_min_index = np.argmin(Min_set)\n",
        "Opt = Opt_set[:, opt_min_index]\n",
        "Opt_df = pd.DataFrame(data=Opt, index = ['Number of Layers', 'Layer Size', 'LR Reuction Rate', 'Patience', 'Batch Size'], columns=['Optimal Hyperparameters'])\n",
        "Opt_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Optimal Hyperparameters</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Number of Layers</th>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Layer Size</th>\n",
              "      <td>200.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>LR Reuction Rate</th>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Patience</th>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Batch Size</th>\n",
              "      <td>64.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                  Optimal Hyperparameters\n",
              "Number of Layers                      3.0\n",
              "Layer Size                          200.0\n",
              "LR Reuction Rate                      0.2\n",
              "Patience                              3.0\n",
              "Batch Size                           64.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JojnJqu-jSi_",
        "outputId": "9e07e1cb-0eab-492b-d722-adb5b01590eb"
      },
      "source": [
        "print(Opt)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[  3.  200.    0.2   3.   64. ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6qcU_GdxiYrS",
        "outputId": "015e201d-01a5-47ea-8872-0ee862b424c1"
      },
      "source": [
        "Opt = [1., 200., 0.2, 3., 64.]\n",
        "print(Opt)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1.0, 200.0, 0.2, 3.0, 64.0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vP26_uQP93RI",
        "outputId": "62cee9bd-1693-492e-adcb-0b4fdeafd49d"
      },
      "source": [
        "if Training:\n",
        "    max_epochs = 200\n",
        "    train = TRAIN(input_dim=input_dim, output_dim=output_dim, max_epochs=max_epochs, state_dict_folder=state_dict_folder)\n",
        "\n",
        "    train.n_layers_optimal, train.hidden_layer_size_optimal, train.lr_reduction_optimal, train.patience_optimal, train.batch_size_optimal = Opt\n",
        "    train.lr_optimal = 0.001\n",
        "    train.n_layers_optimal = int(train.n_layers_optimal)\n",
        "    train.hidden_layer_size_optimal = int(train.hidden_layer_size_optimal)\n",
        "    train.patience_optimal = int(train.patience_optimal)\n",
        "    train.batch_size_optimal = int(train.batch_size_optimal)\n",
        "\n",
        "    print('  Optimal Number of Layer :', train.n_layers_optimal)\n",
        "    print('      Optimal Layer Sizes :', train.hidden_layer_size_optimal)\n",
        "    print('       Optimal Initial LR :', train.lr_optimal)\n",
        "    print('Optimal LR Reduction Rate :', train.lr_reduction_optimal)\n",
        "    print('         Optimal Patience :', train.patience_optimal)\n",
        "    print('        Optimal Bach Size :', train.batch_size_optimal)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  Optimal Number of Layer : 1\n",
            "      Optimal Layer Sizes : 200\n",
            "       Optimal Initial LR : 0.001\n",
            "Optimal LR Reduction Rate : 0.2\n",
            "         Optimal Patience : 3\n",
            "        Optimal Bach Size : 64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "up86OUodnzsR"
      },
      "source": [
        "DEFINING HYPERPARAMETERS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQ1h1mPHGY1P"
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yDabW5oYbQR-",
        "outputId": "1b749a26-cb53-4e34-ba20-8e7855e6c5a9"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True)\n",
        "if Training:\n",
        "    train_split = 0.8\n",
        "    n_layers = train.n_layers_optimal\n",
        "    hidden_layer_size = train.hidden_layer_size_optimal\n",
        "    lr = train.lr_optimal\n",
        "    lr_reduction = train.lr_reduction_optimal\n",
        "    patience = train.patience_optimal\n",
        "    batch_size = train.batch_size_optimal\n",
        "\n",
        "    train.train(X_train, y_train, train_split, n_layers, hidden_layer_size, lr, lr_reduction, patience, batch_size, print_train=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model Trained (Max Epochs).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "noWXuKJA_m39"
      },
      "source": [
        "torch.save({'Layer Number': train.n_layers_optimal,\n",
        "            'Layer Size': train.hidden_layer_size_optimal,\n",
        "            'net_state_dict': train.net.state_dict()\n",
        "            }, state_dict_folder+'/Final_Model_v3.pth')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYiFCnhiihG9"
      },
      "source": [
        "##PREDICTIONS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6jPb_B0QG2Ny",
        "outputId": "831b3aa4-d5c8-46b8-855e-f3af58b05221"
      },
      "source": [
        "checkpoint = torch.load(state_dict_folder+'/Final_Model_v2.pth')\n",
        "n_layers = checkpoint['Layer Number']\n",
        "hidden_layer_size = checkpoint['Layer Size']\n",
        "net = NETWORK(input_dim, output_dim, n_layers=n_layers, hidden_layer_size=hidden_layer_size)\n",
        "net.load_state_dict(checkpoint['net_state_dict'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZqEtmoopbtbN",
        "outputId": "40deeac1-82f6-4d4c-f795-7d01d6cb837c"
      },
      "source": [
        "print(n_layers)\\"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lEladbauID0n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "904cf7c6-3322-4d94-ccbf-5d2f2a34c5e1"
      },
      "source": [
        "testset = TensorDataset(torch.tensor(X_test), torch.tensor(y_test))\n",
        "test_loader = DataLoader(testset, batch_size=124, shuffle=False)\n",
        "print(X.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(43200, 52)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zRxJ7kjUuZag"
      },
      "source": [
        "    def model_error(self, model_test_loader):\n",
        "        MSE = 0\n",
        "        for data in model_test_loader:\n",
        "            X, y = data\n",
        "            prediction = self.net(X.float().to(device))\n",
        "            MSE += np.sum( (prediction.detach().cpu().numpy()-y.detach().cpu().numpy())**2 ) \n",
        "        MSE = MSE / len(model_test_loader.dataset)\n",
        "\n",
        "        return MSE**0.5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63UmvF_sIg28"
      },
      "source": [
        "MSE = 0\n",
        "\n",
        "for data in test_loader:\n",
        "    X_, y_ = data\n",
        "\n",
        "    prediction = net(X_.float().to(device))\n",
        "    \n",
        "    MSE += np.sum( (prediction.detach().cpu().numpy()-y_.detach().cpu().numpy())**2 ) \n",
        "MSE = MSE / len(test_loader.dataset)\n",
        "RMSE = MSE**0.5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6mjENIuOIps2",
        "outputId": "6dc6fc92-e4b1-4c44-bf55-8db362609967"
      },
      "source": [
        "print(RMSE)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.38611365709542383\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}